\name{piclapply}
\alias{piclapply}
%- Also NEED an '\alias' for EACH other topic documented here.
\title{
High performance computing (HPC) parallelization of lapply on PIC
}
\description{
  Parses a list into subsets and submits a separate R job
  using lapply() or mclapply() for each subset. Jobs are executed using parallelized
high performance computing on the PNNL institutional computing cluster
(PIC): olympus.pnl.gov.  The parallel jobs are instantiated
on multiple nodes/cores using a SLURM batch job.
}
\usage{
piclapply(X, FUN, account, ...,
          packages = NULL,
          header.file = NULL,
          needed.objects = NULL,
          needed.objects.env = parent.frame(),
          jobName = "piclapply",
          numNodes = 2,
          partition = c("slurm", "short", "fat", "gpu"),
          time.limit.mins = 30,
          check.interval.sec = 30,
          tarball.file = NULL,
          use.mclapply = FALSE,
          mclapply.args = NULL,
          parseJob.args = NULL, 
          remove.working.dir = TRUE,
          email.notification = NULL,
          verbose = FALSE)
}
%- maybe also 'usage' for other objects documented here.
\arguments{
  \item{X}{
The list, each element of which will be the input to \code{FUN}
}
\item{FUN}{
A function whose first argument is an element of list \code{X}
}
\item{account}{
A character string indicating the PIC account (e.g. "CSI", "SDI",
"USERS") that will charged for the computing time.
}
  \item{\dots}{
Additional named arguments to \code{FUN}
}
  \item{packages}{
Character vector giving the names of packages that
    will be loaded in each new instance of R. If \code{NULL}, no
    packages are loaded for each new instance of R.
}
  \item{header.file}{
Text string indicating a file that will be
    initially sourced prior calling \code{\link{lapply}}
    in order to create an 'environment' that will satisfy all potential
    dependencies for \code{FUN}. If \code{NULL}, no file is
    sourced.
}
  \item{needed.objects}{
Character vector giving the names of objects which reside in the evironment
    specified by \code{needed.objects.env}
    that may be needed by \code{FUN}.  These objects are loaded into the GLOBAL ENVIRONMENT 
    of each new instance of R that is launched.  If \code{NULL}, no
    additional objects are passed the the global environment of the
    various R instances.
}
  \item{needed.objects.env}{
Environment where \code{needed.objects} reside.  
    This defaults to the environment in which \code{plapply} is called.}
  \item{jobName}{
Text string indicating the prefix for files and folders that will
be created while launching the separate instances of R.
}
  \item{numNodes}{
Integer indicating the number of nodes requested for parallel processing.
Each node on olympus has 32 cores.
So, for example, requesting 3 nodes is equivalent to requesting 96 parallel jobs.
}
\item{partition}{Character string indicating the PIC partition to which
  the job should be submitted. May be abbreviated.  Defaults to \code{slurm}, which is the
  partition for 'usual' jobs.  The \code{short} partition is for
  testing and code development for
  short jobs of 1 hour or less. The \code{fat} partition contain nodes
  with more memory, and the \code{gpu} partition contains nodes with GPU
  capability.  Use the system command \code{sview} to see a graphical
  depiction of the entire cluster and to see more detailed information
  about the partitions.}
\item{time.limit.mins}{
  Integer indicating the time limit (in minutes) of the SLURM batch
job. If the SLURM job exceeds this limit it will be canceled.  However,
jobs with shorter limits are likely to be scheduled for launch sooner.
}
  \item{check.interval.sec}{
The number of seconds to wait between
    checking to see whether the SLURM batch job has completed.
}

  \item{tarball.file}{
A text string indicating the file (ending in .tar.gz) where all the files used to
create and support the SLURM job will be stored.  If \code{NULL}, a file of the form
'jobName_output_####.tar.gz' will be stored in the working directory.  If
\code{tarball.file = "none"}, no tarball will be created.
}
\item{use.mclapply}{\code{= TRUE} uses the \code{\link{mclapply}}
  function from the \code{parallel} package to distribute the work to the 32
  cores within a node, and only one instance of R is launched on each
  node. If \code{use.mclapply = FALSE}, a separate instance of R (via a
  system batch call) is launched on each core. See Details.
}
\item{mclapply.args}{A named list of optional (named) arguments to
  \code{\link{mclapply}}.  If \code{mclapply.args = NULL}, the default arguments of
  \code{\link{mclapply}} are used (provided \code{use.mclapply =
    TRUE}).}

\item{parseJob.args}{A named list of optional (named) arguments to
  \code{\link{parseJob}} If \code{parseJob.args = NULL}, the default arguments of
  \code{\link{parseJob}} are used.} 

  \item{remove.working.dir}{
\code{= TRUE} requests the working directory of the SLURM job files 
be deleted on successful completion.  If the SLURM job fails, this directory
will not be removed.
}
  \item{email.notification}{
Text string containing an email address to which an email will be sent upon
completion of the SLURM job.  If \code{NULL}, no email will be sent.
}
  \item{verbose}{
\code{= TRUE} prints details (and timing) of the job 
}
}
\details{
\code{piclapply} applies \code{FUN} to each element of the list \code{X} by parsing
the list into sublists of 
equal (or almost equal) size (using \code{\link{parseJob}}) and then 
applying \code{FUN} to each sublist using \code{\link{lapply}} or
\code{\link{mclapply}}. In the case of the former, the list, \code{X}, will be
parsed using \code{\link{parseJob}} into \code{32*numNodes} sublists,
each of which will be processed by \code{\link{lapply}}
on a separate core in its own instance of R.  For the latter, \code{X} is
parsed into \code{numNodes} sublists, each of which is is processed as a
single instance of R on each node using \code{\link{mclapply}}.

In general, the option \code{use.mclapply = TRUE} should be more efficient
because the R environment only has to be recreated once for each node,
instead of once for each core. But, when using \code{use.mclapply =
  TRUE}, be sure that either 1) the list, \code{X}, is large enough so
that each core will have at least one element of the list to process
(e.g. \code{length(X) >= 32 * numNodes}),
or 2) the computational workload of \code{FUN} is large enough so as to
utilize all 32 cores of each node.  Note that \code{\link{mclapply}}
is capable of recursion--so \code{FUN} could contain a call to
\code{\link{mclapply}}.

For \code{use.mclapply = TRUE}, if the length of
the list is shorter than \code{numNodes}, then the number of nodes is
reduced so that each element of \code{X} is assigned to a single node.
For \code{use.mclapply = FALSE}, if the length of the list is shorter than the number of requested cores
(which is \code{32 * numNodes}), then the number of nodes is reassigned
to a value that will utilize full nodes to the extent possible, and a
warning is displayed.  

After calling \code{piclapply},
it is also good practice to log into one (or more) of the nodes
that are assigned by SLURM and verify (using \code{top}) that all of the
cores are computing as expected.

After the jobs complete, the output lists are reassembled in the
order of the original input list, \code{X}. 

A number of objects are made available in the global environment of each
instance of R.  These objects can be used by \code{FUN} if needed.  They
are:

\itemize{
  
\item \code{process.id}: An integer uniquely identifying the process of R that
  is running.  For \code{use.mclapply = TRUE}, this will range from 0 to
  \code{numNodes - 1}.  For \code{use.mclapply = FALSE}, this will range
  from 0 to \code{32 * numNodes - 1}.
\item \code{num.slurm.processes}: The total number of R instances
  launched by SLURM.  In general, if \code{use.mclapply = TRUE}, then this will be
  equal to \code{numNodes}.  If \code{use.mclapply = FALSE}, then this
  will be \code{32 * numNodes}.
\item \code{wkdir.out}: A character string indicating the directory where
output from each R instance is saved

}

Each instance of R runs a script that performs the following steps:

\enumerate{
  
\item The \code{pnlStat} package is loaded.

\item The \code{parallel} package is loaded if \code{mclapply = TRUE}.

\item Any other packages indicated in the \code{packages} argument are loaded.

\item The \code{process.id} global variable is assigned (having been
passed in via a command line argument).  This variable identifies the
particular instance of R.

\item The header file (if there is one) is sourced.

\item The R environment file is loaded, which contains the list,
\code{X}, the function \code{FUN}, any \code{needed.objects}, as well as
all other objects (internally created by \code{piclapply}) that will be
needed.

\item The expression \code{pre.process.expression} is evaluated if an
object of that name is present in the global environment.
The object \code{pre.process.expression} may be passed in via
the header file or via \code{needed.objects}.

\item The subset of the indexes of the list \code{X} (that will create the
sublist) is identified for the particular instance of R, using the
\code{process.id} variable.

\item The sublist of \code{X} is created and \code{X} is removed to save memory.

\item \code{\link{lapply}} or \code{\link{mclapply}} is called on the sublist.

\item The output returned by \code{\link{lapply}} or \code{\link{mclapply}} is saved to a file in
\code{wkdir.out} where it will be collected after all jobs have completed.

\item The expression \code{post.process.expression} is evaluated if an
object of that name is present in the global environment. 
The object \code{post.process.expression} may be passed in via
the header file or via \code{needed.objects}.

\item Warnings are printed.
}

}
\value{
  A list equivalent to that returned by \code{lapply(X, FUN, ...)}.
}
%\references{
%% ~put references to the literature/web site here ~
%}
\author{
Landon Sego
}
%\note{
%%  ~~further notes~~
%}

%% ~Make other sections like Warning with \section{Warning }{....} ~

\seealso{
\code{\link{lapply}}, \code{\link{mclapply}}, \code{\link{plapply}}, \code{\link{dfplapply}}
} 

\examples{
########################################
# Example 1
########################################

# Note:  you may need to change the account name
account.name <- "USERS"

# Create a simple list
a <- list(a = rnorm(10), b = rnorm(20), c = rnorm(15), d = rnorm(13), e = rnorm(15), f = rnorm(22))
     
# Some objects that will be needed by f1:
b1 <- rexp(20)
b2 <- rpois(10, 20)
     
# The function
f1 <- function(x) mean(x) + max(b1) - min(b2)
     
# Call piclapply
res.1 <- piclapply(a, f1, account.name,
                   needed.objects = c("b1", "b2"),
                   jobName = "example.1",
                   numNodes = 1,
                   partition = "short",
                   check.interval.sec = 1,
                   time.limit.mins = 1,
                   use.mclapply = TRUE,
                   tarball.file = "none",
                   verbose = TRUE)

# Call lapply to check the results
res.2 <- lapply(a, f1)
print(res.2)
     
# Compare results
all.equal(res.1, res.2)


########################################
# Example 2
########################################

# Create a function that calculates the mean of a normal variate
mean.tmp <- function(list.element, sigma = 1) {
  set.seed(list.element)
  mean(rnorm(500, mean = list.element, sd = sigma))
}

# Create a list of means (and seeds) to operate over
aList <- as.list(0:10000)

# Create a useless header file just for demonstration
cat("noUse <- rnorm(100)\n", file = "tmp.hdr.R")

# Use mclapply()
res.3 <- piclapply(aList, mean.tmp, account.name, sigma = 0.5,
                   # These packages aren't really needed.  Just
                   # illustrating how to include them
                   packages = c("MASS", "nlme"),  
                   header.file = "tmp.hdr.R",
                   jobName = "example.2",
                   time.limit.mins = 1,
                   numNodes = 2,
                   partition = "short",
                   # An silly example of controling the parameters for mclapply...
                   use.mclapply = TRUE,
                   mclapply.args = list(mc.cores = 20),
                   check.interval.sec = 1,
                   tarball.file = "none",
                   verbose = TRUE)

unlink("tmp.hdr.R")

# Rerun it, except this time don't use mclapply(), illustrate additional
# arguments to parseJob().  Notice the warning regarding 'nunNodes'
# being too large.
res.4 <- piclapply(aList, mean.tmp, account.name, sigma = 0.5,
                   jobName = "example.2",
                   time.limit.mins = 1,
                   numNodes = 3,
                   partition = "short",
                   parseJob.args = list(collate = TRUE, text.to.eval = TRUE),
                   check.interval.sec = 1,
                   tarball.file = "none",
                   verbose = TRUE)

head(res.3)
tail(res.3)

# These should be the same...
all.equal(res.3, res.4)

}
% Add one or more standard keywords, see file 'KEYWORDS' in the
% R documentation directory.
\keyword{misc}
%\keyword{ ~kwd2 }% __ONLY ONE__ keyword per line
